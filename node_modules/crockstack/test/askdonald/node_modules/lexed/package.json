{
  "_args": [
    [
      "lexed@1.0.10",
      "C:\\Users\\Pino\\Documents\\Work\\CrockStack\\askdonald"
    ]
  ],
  "_from": "lexed@1.0.10",
  "_id": "lexed@1.0.10",
  "_inBundle": false,
  "_integrity": "sha1-aeoQP7ElLQEh5iyRj3kKj2VCBbI=",
  "_location": "/lexed",
  "_phantomChildren": {},
  "_requested": {
    "type": "version",
    "registry": true,
    "raw": "lexed@1.0.10",
    "name": "lexed",
    "escapedName": "lexed",
    "rawSpec": "1.0.10",
    "saveSpec": null,
    "fetchSpec": "1.0.10"
  },
  "_requiredBy": [
    "/finnlp"
  ],
  "_resolved": "https://registry.npmjs.org/lexed/-/lexed-1.0.10.tgz",
  "_spec": "1.0.10",
  "_where": "C:\\Users\\Pino\\Documents\\Work\\CrockStack\\askdonald",
  "author": {
    "name": "Alex Corvi",
    "email": "alex@arrayy.com"
  },
  "bugs": {
    "url": "https://github.com/finnlp/lexed/issues"
  },
  "description": "English word and sentence tokenizer, for natural language processing.",
  "devDependencies": {
    "@types/mocha": "^2.2.39",
    "@types/node": "^7.0.7",
    "ts-node": "^2.1.0",
    "typescript": "^2.2.1"
  },
  "directories": {
    "test": "test"
  },
  "homepage": "https://github.com/finnlp/lexed#readme",
  "keywords": [
    "lexer",
    "lexed",
    "word",
    "sentence",
    "tokenization"
  ],
  "license": "MIT",
  "main": "./dist/index.js",
  "name": "lexed",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/finnlp/lexed.git"
  },
  "scripts": {
    "build": "tsc",
    "penn": "ts-node test/penn-treebank.ts",
    "prepublish": "mocha && tsc",
    "test": "mocha"
  },
  "types": "./dist/index.d.ts",
  "version": "1.0.10"
}
